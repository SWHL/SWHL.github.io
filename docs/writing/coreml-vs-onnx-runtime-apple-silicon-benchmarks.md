---
title: åœ¨ macOS ä¸Šä½¿ç”¨ CoreML åŠ é€Ÿ ONNX æ¨¡å‹æ¨ç†ï¼šå®Œå…¨æŒ‡å—
hide:
  - toc
tags:
  - ONNXRuntime
  - CoreML
comments: true
---

## å‰è¨€

ä½œä¸ºæ·±åº¦å­¦ä¹ å·¥ç¨‹å¸ˆï¼Œæˆ‘ä»¬ç»å¸¸é¢ä¸´ä¸€ä¸ªé—®é¢˜ï¼šå¦‚ä½•åœ¨ Apple è®¾å¤‡ä¸Šé«˜æ•ˆåœ°éƒ¨ç½²å·²æœ‰çš„ ONNX æ¨¡å‹ï¼Ÿæœ¬æ–‡å°†æ·±å…¥æ¢è®¨ä¸¤ç§ä¸»è¦æ–¹æ¡ˆï¼Œå¹¶é€šè¿‡å®é™…æµ‹è¯•ç»™å‡ºè¯¦ç»†çš„æ€§èƒ½å’Œç²¾åº¦å¯¹æ¯”ã€‚

## ç›®å½•

1. [èƒŒæ™¯ï¼šä¸ºä»€ä¹ˆè¦ç”¨ CoreML](#èƒŒæ™¯)
2. [æ–¹æ¡ˆå¯¹æ¯”ï¼šONNX Runtime vs åŸç”Ÿ CoreML](#æ–¹æ¡ˆå¯¹æ¯”)
3. [æ€§èƒ½æµ‹è¯•ï¼šMobileNetV3 å®æˆ˜](#æ€§èƒ½æµ‹è¯•)
4. [ONNX è½¬ CoreMLï¼šå¯è¡Œæ€§åˆ†æ](#è½¬æ¢æ–¹æ¡ˆ)
5. [æœ€ä½³å®è·µå»ºè®®](#æœ€ä½³å®è·µ)

---

## èƒŒæ™¯ï¼šä¸ºä»€ä¹ˆè¦ç”¨ CoreML

CoreML æ˜¯ Apple çš„æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œé’ˆå¯¹ Apple èŠ¯ç‰‡ï¼ˆç‰¹åˆ«æ˜¯ Neural Engineï¼‰è¿›è¡Œäº†æ·±åº¦ä¼˜åŒ–ã€‚ä½¿ç”¨ CoreML çš„ä¸»è¦ä¼˜åŠ¿ï¼š

- âš¡ **æ€§èƒ½ä¼˜åŒ–**ï¼šå……åˆ†åˆ©ç”¨ Neural Engine å’Œ GPU
- ğŸ”‹ **èƒ½æ•ˆæ¯”é«˜**ï¼šæ›´ä½çš„åŠŸè€—ï¼Œé€‚åˆç§»åŠ¨è®¾å¤‡
- ğŸ“¦ **é›†æˆç®€å•**ï¼šåŸç”Ÿæ”¯æŒ iOS/macOS å¼€å‘
- ğŸ›¡ï¸ **éšç§ä¿æŠ¤**ï¼šæ¨¡å‹åœ¨æœ¬åœ°è¿è¡Œï¼Œæ•°æ®ä¸ä¸Šä¼ 

ä½†é—®é¢˜æ¥äº†ï¼š**å¦‚æœæˆ‘ä»¬å·²ç»æœ‰äº† ONNX æ¨¡å‹ï¼Œå¦‚ä½•åœ¨ Apple è®¾å¤‡ä¸Šä½¿ç”¨ CoreML åŠ é€Ÿï¼Ÿ**

---

## æ–¹æ¡ˆå¯¹æ¯”ï¼šONNX Runtime vs åŸç”Ÿ CoreML

### æ–¹æ¡ˆ 1ï¼šONNX Runtime + CoreML Provider

ç›´æ¥ä½¿ç”¨ ONNX Runtime çš„ CoreML Execution Providerï¼Œæ— éœ€æ‰‹åŠ¨è½¬æ¢ã€‚

```python linenums="1"
import onnxruntime as ort

session = ort.InferenceSession(
    'model.onnx',
    providers=['CoreMLExecutionProvider', 'CPUExecutionProvider']
)

# æ¨ç†
input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)
output = session.run(None, {'input': input_data})
```

**å·¥ä½œåŸç†ï¼š**

- ONNX Runtime åœ¨é¦–æ¬¡åŠ è½½æ—¶ä¼šå°† ONNX æ¨¡å‹å³æ—¶è½¬æ¢ä¸º CoreML
- è½¬æ¢ç»“æœä¼šç¼“å­˜åˆ° `~/Library/Caches/onnxruntime`
- åç»­åŠ è½½ä¼šç›´æ¥ä½¿ç”¨ç¼“å­˜çš„ CoreML æ¨¡å‹

**ä¼˜ç‚¹ï¼š**

- âœ… è·¨å¹³å°å…¼å®¹ï¼ˆWindows/Linux/macOSï¼‰
- âœ… ä¸€ä¸ª ONNX æ–‡ä»¶ï¼Œå¤šå¹³å°é€šç”¨
- âœ… æ— éœ€æ‰‹åŠ¨è½¬æ¢
- âœ… è‡ªåŠ¨å›é€€æœºåˆ¶ï¼ˆä¸æ”¯æŒçš„ç®—å­å›é€€åˆ° CPUï¼‰
- âœ… å¼€å‘ä½“éªŒå¥½

**ç¼ºç‚¹ï¼š**

- âŒ é¦–æ¬¡åŠ è½½æœ‰è½¬æ¢å¼€é”€ï¼ˆçº¦ 0.5 ç§’ï¼‰
- âŒ è¿è¡Œæ—¶æ€§èƒ½ä¸å¦‚åŸç”Ÿ CoreML
- âŒ å¯èƒ½ä¸æ”¯æŒæ‰€æœ‰ ONNX ç®—å­

---

### æ–¹æ¡ˆ 2ï¼šé¢„è½¬æ¢ä¸º CoreML æ ¼å¼

å°†æ¨¡å‹é¢„å…ˆè½¬æ¢ä¸º `.mlpackage` æˆ– `.mlmodel` æ ¼å¼åˆ†å‘ã€‚

```python linenums="1"
import torch
import coremltools as ct

# ä» PyTorch åŠ è½½æ¨¡å‹
model = torch.jit.load('model.pt')
model.eval()

# è½¬æ¢ä¸º CoreML
example_input = torch.rand(1, 3, 224, 224)
traced_model = torch.jit.trace(model, example_input)

mlmodel = ct.convert(
    traced_model,
    inputs=[ct.TensorType(name="input", shape=(1, 3, 224, 224))],
    convert_to='mlprogram',
    compute_units=ct.ComputeUnit.ALL,
)

mlmodel.save('model.mlpackage')
```

**ä½¿ç”¨ï¼š**

```python linenums="1"
import coremltools as ct

model = ct.models.MLModel('model.mlpackage')
output = model.predict({'input': input_data})
```

**ä¼˜ç‚¹ï¼š**

- âœ… æ¨ç†é€Ÿåº¦æœ€å¿«
- âœ… åŠ è½½åå³å¯ä½¿ç”¨ï¼Œæ— è½¬æ¢å¼€é”€
- âœ… æ¨¡å‹æ–‡ä»¶æ›´å°
- âœ… å¯åœ¨ Xcode ä¸­é¢„è§ˆå’Œè°ƒè¯•
- âœ… iOS/macOS åŸç”Ÿåº”ç”¨å¯ç›´æ¥ä½¿ç”¨

**ç¼ºç‚¹ï¼š**

- âŒ ä»…æ”¯æŒ Apple è®¾å¤‡
- âŒ éœ€è¦ç»´æŠ¤å¤šä¸ªæ ¼å¼ï¼ˆONNX + CoreMLï¼‰
- âŒ è½¬æ¢è¿‡ç¨‹å¯èƒ½å¤±è´¥
- âŒ éœ€è¦é¢å¤–çš„è½¬æ¢å’ŒéªŒè¯æµç¨‹

---

## æ€§èƒ½æµ‹è¯•ï¼šMobileNetV3 å®æˆ˜

### æµ‹è¯•ç¯å¢ƒ

- **ç¡¬ä»¶**ï¼šMacBookï¼ˆApple Siliconï¼‰
- **ç³»ç»Ÿ**ï¼šmacOS
- **æ¨¡å‹**ï¼šMobileNetV3-Small (9.71 MB)
- **è¾“å…¥**ï¼š1Ã—3Ã—224Ã—224 float32
- **æ¡†æ¶ç‰ˆæœ¬**ï¼š
    - ONNX Runtime: 1.22.0
    - CoreMLTools: 8.3.0

### æµ‹è¯•ä»£ç 

å®Œæ•´æµ‹è¯•ä»£ç å·²å¼€æºï¼ˆ[Gist](https://gist.github.com/SWHL/fd48ffee4b39ece6f8c418031915872f)ï¼‰ï¼ŒåŒ…å«ä»¥ä¸‹åŠŸèƒ½ï¼š

1. CoreML Provider å¯ç”¨æ€§æ£€æµ‹
2. æ€§èƒ½å¯¹æ¯”æµ‹è¯•ï¼ˆ100 æ¬¡æ¨ç†ï¼‰
3. ç²¾åº¦éªŒè¯ï¼ˆå¤šæ ·æœ¬å¯¹æ¯”ï¼‰
4. Top-5 é¢„æµ‹ä¸€è‡´æ€§æ£€æŸ¥

```bash
# æµ‹è¯• ONNX Runtime + CoreML Provider
python test_coreml.py

# å¯¹æ¯”ä¸¤ç§æ–¹æ¡ˆ
python compare_coreml_methods.py
```

---

### æµ‹è¯•ç»“æœ 1ï¼šONNX Runtime (CoreML vs CPU)

é¦–å…ˆæµ‹è¯• ONNX Runtime ä¸­ CoreML Provider å’Œ CPU Provider çš„æ€§èƒ½å·®å¼‚ï¼š

| æŒ‡æ ‡ | CoreML Provider | CPU Provider | å¯¹æ¯” |
|------|----------------|-------------|------|
| **å¹³å‡æ¨ç†æ—¶é—´** | 7.466 ms | 5.202 ms | CPU å¿« 30% |
| **æ ‡å‡†å·®** | 0.562 ms | 3.593 ms | CoreML æ›´ç¨³å®š |
| **æœ€å°å»¶è¿Ÿ** | 6.371 ms | 4.161 ms | - |
| **P95 å»¶è¿Ÿ** | 8.676 ms | 6.938 ms | - |
| **P99 å»¶è¿Ÿ** | 9.328 ms | 13.213 ms | CoreML æ›´å¥½ |
| **ååé‡** | 133.94 FPS | 192.22 FPS | CPU æ›´é«˜ |

**å…³é”®å‘ç°ï¼š**

- ğŸŒ åœ¨ ONNX Runtime ä¸­ï¼ŒCoreML Provider åè€Œæ¯” CPU æ…¢ 30%
- âœ… ä½† CoreML çš„å»¶è¿Ÿæ›´ç¨³å®šï¼ˆæ ‡å‡†å·®å° 6 å€ï¼‰
- âš ï¸ CPU å¶å°”ä¼šå‡ºç°æç«¯å»¶è¿Ÿï¼ˆæœ€å¤§ 39.3msï¼‰

**åŸå› åˆ†æï¼š**

- ONNX Runtime çš„ CoreML Provider æœ‰é¢å¤–çš„è½¬æ¢å’Œæ•°æ®ä¼ è¾“å¼€é”€
- MobileNetV3 æ¨¡å‹è¾ƒå°ï¼Œè½¬æ¢å¼€é”€å æ¯”å¤§
- CPU Provider ä½¿ç”¨äº†é«˜åº¦ä¼˜åŒ–çš„ BLAS åº“ï¼ˆå¯èƒ½æ˜¯ Apple Accelerateï¼‰

---

### æµ‹è¯•ç»“æœ 2ï¼šåŸç”Ÿ CoreML vs ONNX Runtime

å¯¹æ¯”åŸç”Ÿ CoreML å’Œ ONNX Runtime + CoreML Providerï¼š

| æŒ‡æ ‡ | ONNX Runtime + CoreML | åŸç”Ÿ CoreML | å·®å¼‚ |
|------|----------------------|------------|------|
| **åŠ è½½æ—¶é—´** | 0.555 ç§’ | 0.974 ç§’ | ONNX æ›´å¿« |
| **é¦–æ¬¡æ¨ç†** | 0.013 ç§’ | 0.023 ç§’ | ONNX æ›´å¿« |
| **å¹³å‡æ¨ç†æ—¶é—´** | **6.447 ms** | **0.313 ms** | ğŸš€ **åŸç”Ÿå¿« 20.6 å€** |
| **æ¨ç†æ ‡å‡†å·®** | 0.296 ms | 0.040 ms | åŸç”Ÿæ›´ç¨³å®š |
| **æ¨¡å‹å¤§å°** | 9.71 MB | 4.95 MB | åŸç”Ÿå° 49% |

**æ€§èƒ½å¯è§†åŒ–ï¼š**

```text
æ¨ç†æ—¶é—´å¯¹æ¯”ï¼ˆè¶ŠçŸ­è¶Šå¥½ï¼‰
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ONNX Runtime + CoreML  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 6.447 ms              â”‚
â”‚ åŸç”Ÿ CoreML            â–ˆ 0.313 ms                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†‘ 20.6x åŠ é€Ÿï¼
```

**å…³é”®å‘ç°ï¼š**

- ğŸš€ **åŸç”Ÿ CoreML æ¨ç†é€Ÿåº¦å¿« 20.6 å€**
- ğŸ“¦ æ¨¡å‹ä½“ç§¯å‡å°‘ 49%ï¼ˆ9.71 MB â†’ 4.95 MBï¼‰
- âœ… å»¶è¿Ÿæ›´ç¨³å®šï¼ˆÂ±0.040ms vs Â±0.296msï¼‰
- âš ï¸ ä½†åŠ è½½æ—¶é—´ç¨æ…¢ï¼ˆå¤š 0.4 ç§’ï¼‰

**æ€§èƒ½åˆ†å¸ƒå¯¹æ¯”ï¼š**

```text
æ¨ç†æ—¶é—´åˆ†å¸ƒï¼š
  ONNX Runtime:  min=6.371ms, p50=7.401ms, p95=8.676ms, p99=9.328ms
  åŸç”Ÿ CoreML:   min=0.273ms, p50=0.303ms, p95=0.393ms, p99=0.433ms
```

---

### ç²¾åº¦å¯¹æ¯”

æµ‹è¯• 10 ä¸ªéšæœºæ ·æœ¬ï¼Œå¯¹æ¯”è¾“å‡ºå·®å¼‚ï¼š

**ONNX Runtime (CoreML vs CPU)ï¼š**

- å¹³å‡æœ€å¤§å·®å¼‚: 6.01e-02ï¼ˆçº¦ 6%ï¼‰
- å¹³å‡å¹³å‡å·®å¼‚: 1.30e-02ï¼ˆçº¦ 1.3%ï¼‰
- Top-5 é¢„æµ‹ä¸€è‡´æ€§: 2/3 æ ·æœ¬å®Œå…¨ç›¸åŒ
- **ç»“è®º**: âš ï¸ å­˜åœ¨è½»å¾®å·®å¼‚ï¼Œä½†å¤§éƒ¨åˆ†é¢„æµ‹ä¸€è‡´

**åŸç”Ÿ CoreML vs ONNX Runtime (CPU)ï¼š**

- å¹³å‡æœ€å¤§å·®å¼‚: 1.01
- å¹³å‡å¹³å‡å·®å¼‚: 0.22
- Top-5 é¢„æµ‹ä¸€è‡´æ€§: 2/3 æ ·æœ¬å®Œå…¨ç›¸åŒ
- **ç»“è®º**: âš ï¸ å·®å¼‚è¾ƒå¤§ï¼Œéœ€è¦åœ¨å®é™…åº”ç”¨ä¸­éªŒè¯æ˜¯å¦å¯æ¥å—

**ç²¾åº¦å·®å¼‚åŸå› ï¼š**

1. æµ®ç‚¹è¿ç®—é¡ºåºä¸åŒ
2. CoreML å¯èƒ½ä½¿ç”¨ä½ç²¾åº¦è®¡ç®—ï¼ˆFP16ï¼‰
3. ç®—å­å®ç°å·®å¼‚
4. ä¼˜åŒ–å’Œèåˆç­–ç•¥ä¸åŒ

---

## ONNX è½¬ CoreMLï¼šå¯è¡Œæ€§åˆ†æ

### ç†æƒ³æ–¹æ¡ˆï¼šç›´æ¥è½¬æ¢

ç†è®ºä¸Šï¼Œæˆ‘ä»¬å¸Œæœ›ï¼š

```bash
# ç†æƒ³ä½†ä¸å¯è¡Œ âŒ
onnx-model.onnx â†’ CoreML è½¬æ¢å·¥å…· â†’ model.mlpackage
```

### å®é™…æƒ…å†µï¼šè½¬æ¢å·¥å…·ç°çŠ¶

ç»è¿‡å®æµ‹ï¼Œç›®å‰çš„è½¬æ¢å·¥å…·çŠ¶æ€å¦‚ä¸‹ï¼š

#### 1. onnx-coremlï¼ˆå·²å¼ƒç”¨ï¼‰âŒ

```bash
pip install onnx-coreml
```

**çŠ¶æ€ï¼š** å·²åœæ­¢ç»´æŠ¤ï¼Œä¸æ–°ç‰ˆ coremltools ä¸å…¼å®¹

**é”™è¯¯ä¿¡æ¯ï¼š**

```text
ModuleNotFoundError: No module named 'coremltools.converters.nnssa'
```

**åŸå› ï¼š** coremltools 7.0+ é‡æ„äº†å†…éƒ¨ APIï¼Œç§»é™¤äº† `nnssa` æ¨¡å—

**ç»“è®ºï¼š** âŒ ä¸æ¨èä½¿ç”¨

---

#### 2. coremltools ç›´æ¥è½¬æ¢ï¼ˆå·²ç§»é™¤ï¼‰âŒ

```python linenums="1"
import coremltools as ct

# å°è¯•ç›´æ¥è½¬æ¢ ONNX
mlmodel = ct.convert('model.onnx', source='onnx')
```

**é”™è¯¯ä¿¡æ¯ï¼š**

```text
ValueError: Unrecognized value of argument "source": onnx.
It must be one of ["auto", "tensorflow", "pytorch", "milinternal"].
```

**åŸå› ï¼š** coremltools 8.x å·²ç»ç§»é™¤äº†å¯¹ ONNX çš„ç›´æ¥æ”¯æŒ

**ç»“è®ºï¼š** âŒ ä¸å¯è¡Œ

---

#### 3. é€šè¿‡åŸå§‹æ¡†æ¶è½¬æ¢ï¼ˆå”¯ä¸€å¯è¡Œï¼‰âœ…

```python linenums="1"
import torch
import coremltools as ct

# 1. åŠ è½½ PyTorch æ¨¡å‹
model = torch.jit.load('model.pt')
model.eval()

# 2. è¿½è¸ªæ¨¡å‹
example_input = torch.rand(1, 3, 224, 224)
traced_model = torch.jit.trace(model, example_input)

# 3. è½¬æ¢ä¸º CoreML
mlmodel = ct.convert(
    traced_model,
    inputs=[ct.TensorType(name="input", shape=(1, 3, 224, 224))],
    convert_to='mlprogram',
    compute_units=ct.ComputeUnit.ALL,
)

# 4. ä¿å­˜
mlmodel.save('model.mlpackage')
```

**è½¬æ¢æ€§èƒ½ï¼š**

- è½¬æ¢æ—¶é—´: 3.3 ç§’
- æ¨¡å‹å¤§å°: 9.71 MB â†’ 4.95 MBï¼ˆå‡å°‘ 49%ï¼‰

**ç»“è®ºï¼š** âœ… **ç›®å‰å”¯ä¸€å¯è¡Œçš„æ–¹æ¡ˆ**

---

### è½¬æ¢æ–¹æ¡ˆæ€»ç»“

| æ–¹æ¡ˆ | å¯è¡Œæ€§ | è¯´æ˜ |
|------|--------|------|
| **onnx-coreml** | âŒ å·²å¼ƒç”¨ | ä¸æ–°ç‰ˆ coremltools ä¸å…¼å®¹ |
| **coremltools ç›´æ¥è½¬ ONNX** | âŒ å·²ç§»é™¤ | coremltools 8.x ä¸å†æ”¯æŒ |
| **PyTorch â†’ CoreML** | âœ… æ¨è | éœ€è¦åŸå§‹ PyTorch æ¨¡å‹ |
| **TensorFlow â†’ CoreML** | âœ… å¯ç”¨ | éœ€è¦åŸå§‹ TF æ¨¡å‹ |
| **ONNX Runtime + CoreML Provider** | âœ… æ›¿ä»£æ–¹æ¡ˆ | è¿è¡Œæ—¶è½¬æ¢ï¼Œæ— éœ€æ‰‹åŠ¨è½¬æ¢ |

---

## æœ€ä½³å®è·µå»ºè®®

### åœºæ™¯ 1ï¼šåªæœ‰ ONNX æ¨¡å‹ï¼Œæ— åŸå§‹ä»£ç 

**æ¨èæ–¹æ¡ˆï¼š** ONNX Runtime + CoreML Provider

```python linenums="1"
import onnxruntime as ort

# åˆ›å»º sessionï¼Œè‡ªåŠ¨ä½¿ç”¨ CoreML
session = ort.InferenceSession(
    'model.onnx',
    providers=['CoreMLExecutionProvider', 'CPUExecutionProvider']
)

# æ¨ç†
output = session.run(None, {'input': input_data})
```

**é€‚ç”¨åœºæ™¯ï¼š**

- âœ… éœ€è¦è·¨å¹³å°éƒ¨ç½²
- âœ… æ¨¡å‹é¢‘ç¹æ›´æ–°
- âœ… è¿½æ±‚å¼€å‘æ•ˆç‡
- âš ï¸ å¯ä»¥æ¥å—è¾ƒæ…¢çš„æ¨ç†é€Ÿåº¦ï¼ˆæ¯”åŸç”Ÿæ…¢ 20 å€ï¼‰

---

### åœºæ™¯ 2ï¼šæœ‰åŸå§‹ PyTorch/TensorFlow æ¨¡å‹

**æ¨èæ–¹æ¡ˆï¼š** ç›´æ¥è½¬æ¢ä¸º CoreML

```python linenums="1"
# è½¬æ¢è„šæœ¬
import torch
import coremltools as ct

def convert_to_coreml(pytorch_model_path, output_path):
    # åŠ è½½æ¨¡å‹
    model = torch.jit.load(pytorch_model_path)
    model.eval()

    # è¿½è¸ª
    example_input = torch.rand(1, 3, 224, 224)
    traced_model = torch.jit.trace(model, example_input)

    # è½¬æ¢
    mlmodel = ct.convert(
        traced_model,
        inputs=[ct.TensorType(name="input", shape=(1, 3, 224, 224))],
        convert_to='mlprogram',
        compute_units=ct.ComputeUnit.ALL,
    )

    # ä¿å­˜
    mlmodel.save(output_path)
    print(f"âœ… å·²ä¿å­˜åˆ°: {output_path}")

# ä½¿ç”¨
convert_to_coreml('model.pt', 'model.mlpackage')
```

**é€‚ç”¨åœºæ™¯ï¼š**

- âœ… ä»…æ”¯æŒ Apple è®¾å¤‡
- âœ… è¿½æ±‚æè‡´æ€§èƒ½ï¼ˆå¿« 20 å€ï¼‰
- âœ… ç§»åŠ¨ç«¯åº”ç”¨ï¼ˆæ¨¡å‹æ›´å°ï¼‰
- âœ… ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

---

### åœºæ™¯ 3ï¼šè·¨å¹³å°æ¡Œé¢åº”ç”¨

**æ¨èæ–¹æ¡ˆï¼š** ONNX Runtimeï¼Œæ ¹æ®å¹³å°é€‰æ‹© Provider

```python linenums="1"
import onnxruntime as ort
import platform

# æ ¹æ®å¹³å°é€‰æ‹©æœ€ä½³ provider
def get_best_providers():
    system = platform.system()
    if system == 'Darwin':  # macOS
        return ['CoreMLExecutionProvider', 'CPUExecutionProvider']
    elif system == 'Windows':
        return ['DmlExecutionProvider', 'CPUExecutionProvider']
    else:  # Linux
        return ['CUDAExecutionProvider', 'CPUExecutionProvider']

session = ort.InferenceSession('model.onnx', providers=get_best_providers())
```

---

### åœºæ™¯ 4ï¼šiOS/macOS åŸç”Ÿåº”ç”¨

**æ¨èæ–¹æ¡ˆï¼š** ä½¿ç”¨ CoreMLï¼ˆSwift/Objective-Cï¼‰

```swift
import CoreML

// åŠ è½½æ¨¡å‹
guard let model = try? MobileNetV3() else {
    fatalError("Failed to load model")
}

// å‡†å¤‡è¾“å…¥
let input = MobileNetV3Input(input: inputImage)

// æ¨ç†
if let output = try? model.prediction(input: input) {
    print("Prediction: \\(output)")
}
```

---

### åœºæ™¯ 5ï¼šå®æ—¶åº”ç”¨ï¼ˆAR/VR/ç›´æ’­ï¼‰

**æ¨èæ–¹æ¡ˆï¼š** åŸç”Ÿ CoreML

**åŸå› ï¼š**

- æ¨ç†å»¶è¿Ÿ 0.3 msï¼ˆvs ONNX Runtime 6.4 msï¼‰
- å»¶è¿Ÿç¨³å®šï¼ŒP99 < 0.5 ms
- æ›´ä½çš„åŠŸè€—

---

## æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. å›ºå®šè¾“å…¥å½¢çŠ¶

åŠ¨æ€å½¢çŠ¶ä¼šé™ä½æ€§èƒ½ï¼Œå»ºè®®å›ºå®šï¼š

```python linenums="1"
import onnx
from onnx import shape_inference

# åŠ è½½æ¨¡å‹
model = onnx.load('model.onnx')

# æ¨æ–­å½¢çŠ¶
inferred_model = shape_inference.infer_shapes(model)

# ä¿å­˜
onnx.save(inferred_model, 'model_fixed.onnx')
```

### 2. ä½¿ç”¨ FP16 é‡åŒ–

å‡å°æ¨¡å‹å¤§å°ï¼Œæå‡é€Ÿåº¦ï¼š

```python linenums="1"
import coremltools as ct

mlmodel = ct.models.MLModel('model.mlpackage')

# é‡åŒ–ä¸º FP16
mlmodel_fp16 = ct.models.neural_network.quantization_utils.quantize_weights(
    mlmodel, nbits=16
)

mlmodel_fp16.save('model_fp16.mlpackage')
```

### 3. æ¨¡å‹é¢„çƒ­

é¦–æ¬¡æ¨ç†è¾ƒæ…¢ï¼Œå»ºè®®é¢„çƒ­ï¼š

```python linenums="1"
# é¢„çƒ­
for _ in range(10):
    session.run(None, {'input': dummy_input})

# æ­£å¼æ¨ç†
output = session.run(None, {'input': real_input})
```

### 4. æ‰¹å¤„ç†

å¦‚æœåœºæ™¯å…è®¸ï¼Œä½¿ç”¨æ‰¹å¤„ç†æå‡ååï¼š

```python linenums="1"
# æ‰¹é‡æ¨ç†
batch_input = np.stack([img1, img2, img3, img4])  # (4, 3, 224, 224)
outputs = session.run(None, {'input': batch_input})
```

---

## å¸¸è§é—®é¢˜ FAQ

### Q1: ä¸ºä»€ä¹ˆ ONNX Runtime çš„ CoreML Provider è¿™ä¹ˆæ…¢ï¼Ÿ

**A:** ä¸»è¦æœ‰ä¸‰ä¸ªåŸå› ï¼š

1. **è½¬æ¢å¼€é”€**ï¼šå³ä½¿æœ‰ç¼“å­˜ï¼Œä»æœ‰æ•°æ®æ ¼å¼è½¬æ¢
2. **æ¨¡å‹å¤ªå°**ï¼šMobileNetV3 å¤ªå°ï¼Œè½¬æ¢å¼€é”€å æ¯”å¤§
3. **ä¼˜åŒ–ä¸è¶³**ï¼šONNX Runtime çš„ CoreML Provider è¿˜ä¸å¤Ÿæˆç†Ÿ

**å»ºè®®**ï¼šå¯¹äºå¤§æ¨¡å‹ï¼ˆå¦‚ ResNet50ï¼‰ï¼ŒCoreML Provider çš„ä¼˜åŠ¿ä¼šæ›´æ˜æ˜¾ã€‚

---

### Q2: åŸç”Ÿ CoreML çš„ç²¾åº¦ä¸ºä»€ä¹ˆä¼šæœ‰å·®å¼‚ï¼Ÿ

**A:** ä¸»è¦åŸå› ï¼š

1. CoreML å¯èƒ½ä½¿ç”¨ FP16ï¼ˆåŠç²¾åº¦ï¼‰è®¡ç®—
2. ç®—å­èåˆå’Œä¼˜åŒ–å¯¼è‡´è®¡ç®—é¡ºåºä¸åŒ
3. æŸäº›ç®—å­çš„å®ç°å·®å¼‚

**å»ºè®®**ï¼š

- åœ¨å®é™…æ•°æ®ä¸ŠéªŒè¯ç²¾åº¦
- å¯¹æ¯” Top-5/Top-10 é¢„æµ‹çš„ä¸€è‡´æ€§
- å¦‚æœå·®å¼‚è¿‡å¤§ï¼Œå¯ä»¥å…³é—­æŸäº›ä¼˜åŒ–

---

### Q3: å¦‚ä½•é€‰æ‹© CoreML çš„è®¡ç®—å•å…ƒï¼Ÿ

```python linenums="1"
import coremltools as ct

# æ‰€æœ‰å¯ç”¨å•å…ƒï¼ˆæ¨èï¼‰
compute_units=ct.ComputeUnit.ALL

# ä»… CPU
compute_units=ct.ComputeUnit.CPU_ONLY

# CPU + GPU
compute_units=ct.ComputeUnit.CPU_AND_GPU

# CPU + Neural Engine
compute_units=ct.ComputeUnit.CPU_AND_NE
```

**å»ºè®®**ï¼š

- å¼€å‘è°ƒè¯•ï¼šä½¿ç”¨ `CPU_ONLY`ï¼ˆç»“æœç¡®å®šï¼‰
- ç”Ÿäº§éƒ¨ç½²ï¼šä½¿ç”¨ `ALL`ï¼ˆæœ€å¿«ï¼‰

---

### Q4: CoreML æ¨¡å‹å¯ä»¥åœ¨ Windows/Linux ä¸Šè¿è¡Œå—ï¼Ÿ

**A:** âŒ ä¸å¯ä»¥ã€‚CoreML æ˜¯ Apple çš„ä¸“æœ‰æ¡†æ¶ï¼Œä»…æ”¯æŒ macOS/iOS/iPadOS/tvOS/watchOSã€‚

å¦‚æœéœ€è¦è·¨å¹³å°ï¼Œä½¿ç”¨ ONNX Runtimeã€‚

---

### Q5: å¦‚ä½•åœ¨ Python ä¸­ä½¿ç”¨ .mlpackage æ¨¡å‹ï¼Ÿ

```python linenums="1"
import coremltools as ct
import numpy as np

# åŠ è½½æ¨¡å‹
model = ct.models.MLModel('model.mlpackage')

# æŸ¥çœ‹è¾“å…¥è¾“å‡º
spec = model.get_spec()
print("è¾“å…¥:", spec.description.input[0].name)
print("è¾“å‡º:", spec.description.output[0].name)

# æ¨ç†
input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)
output = model.predict({'input': input_data})
print("ç»“æœ:", output)
```

---

## å®Œæ•´æµ‹è¯•ä»£ç 

æ‰€æœ‰æµ‹è¯•ä»£ç å·²æ•´ç†å¹¶æ·»åŠ è¯¦ç»†æ³¨é‡Šï¼š

### 1. [test_coreml.py](https://gist.github.com/SWHL/fd48ffee4b39ece6f8c418031915872f#file-test_coreml-py) - CoreML å¯ç”¨æ€§å’Œæ€§èƒ½æµ‹è¯•

æµ‹è¯• ONNX Runtime çš„ CoreML Provider æ˜¯å¦å¯ç”¨ï¼Œå¹¶å¯¹æ¯” CoreML å’Œ CPU çš„æ€§èƒ½ã€‚

```bash
python test_coreml.py
```

**åŠŸèƒ½ï¼š**

- âœ… æ£€æµ‹ CoreML Provider å¯ç”¨æ€§
- âœ… åˆ›å»ºæµ‹è¯• ONNX æ¨¡å‹
- âœ… å¯¹æ¯” CoreML å’Œ CPU æ€§èƒ½
- âœ… éªŒè¯è¾“å‡ºç²¾åº¦

---

### 2. [compare_coreml_methods.py](https://gist.github.com/SWHL/fd48ffee4b39ece6f8c418031915872f#file-compare_coreml_methods-py) - ä¸¤ç§æ–¹æ¡ˆå®Œæ•´å¯¹æ¯”

å¯¹æ¯” ONNX Runtime + CoreML Provider å’ŒåŸç”Ÿ CoreML çš„æ€§èƒ½å’Œç²¾åº¦ã€‚

```bash
python compare_coreml_methods.py
```

**åŠŸèƒ½ï¼š**

- âœ… ä» PyTorch å¯¼å‡º/è½¬æ¢æ¨¡å‹
- âœ… å¯¹æ¯”åŠ è½½æ—¶é—´
- âœ… å¯¹æ¯”æ¨ç†æ€§èƒ½ï¼ˆå¹³å‡/P50/P95/P99ï¼‰
- âœ… å¯¹æ¯”æ¨¡å‹å¤§å°
- âœ… ç²¾åº¦éªŒè¯ï¼ˆå¤šæ ·æœ¬ï¼‰
- âœ… Top-5 é¢„æµ‹ä¸€è‡´æ€§æ£€æŸ¥

---

### 3. [onnx_to_coreml_modern.py](https://gist.github.com/SWHL/fd48ffee4b39ece6f8c418031915872f#file-onnx_to_coreml_modern-py) - ONNX è½¬æ¢å·¥å…·

å°è¯•å°† ONNX æ¨¡å‹è½¬æ¢ä¸º CoreMLï¼ˆä¼šå¤±è´¥ï¼Œç”¨äºæ¼”ç¤ºå½“å‰é™åˆ¶ï¼‰ã€‚

```bash
python onnx_to_coreml_modern.py model.onnx --info-only
```

**åŠŸèƒ½ï¼š**

- âœ… æ˜¾ç¤º ONNX æ¨¡å‹è¯¦ç»†ä¿¡æ¯
- âœ… å°è¯•è½¬æ¢ï¼ˆæ¼”ç¤ºä¸å¯è¡Œï¼‰
- âœ… ç²¾åº¦éªŒè¯
- âœ… å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæç¤º

---

## ç»“è®º

ç»è¿‡è¯¦ç»†çš„æµ‹è¯•å’Œåˆ†æï¼Œæˆ‘ä»¬å¾—å‡ºä»¥ä¸‹ç»“è®ºï¼š

### æ€§èƒ½å¯¹æ¯”æ€»ç»“

| æ–¹æ¡ˆ | æ¨ç†é€Ÿåº¦ | æ¨¡å‹å¤§å° | è·¨å¹³å° | æ˜“ç”¨æ€§ | æ¨èåº¦ |
|------|---------|---------|--------|--------|--------|
| **ONNX Runtime (CPU)** | 5.2 ms | 9.71 MB | âœ… | â­â­â­â­â­ | å¼€å‘é¦–é€‰ |
| **ONNX Runtime (CoreML)** | 6.4 ms | 9.71 MB | âœ… | â­â­â­â­ | - |
| **åŸç”Ÿ CoreML** | **0.3 ms** ğŸš€ | **4.95 MB** | âŒ | â­â­â­ | ç”Ÿäº§é¦–é€‰ |

### æ ¸å¿ƒå‘ç°

1. **åŸç”Ÿ CoreML æ€§èƒ½æä½³**ï¼šæ¯” ONNX Runtime å¿« **20.6 å€**
2. **ONNX Runtime çš„ CoreML Provider ä¸ç†æƒ³**ï¼šåè€Œæ¯” CPU æ…¢ 30%
3. **ç›´æ¥è½¬æ¢ ONNX â†’ CoreML ä¸å¯è¡Œ**ï¼šå¿…é¡»é€šè¿‡åŸå§‹æ¡†æ¶
4. **ç²¾åº¦æœ‰å·®å¼‚**ï¼šéœ€è¦åœ¨å®é™…åº”ç”¨ä¸­éªŒè¯æ˜¯å¦å¯æ¥å—

### é€‰æ‹©å»ºè®®

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  éœ€è¦è·¨å¹³å°ï¼Ÿ                                              â”‚
â”‚    â”œâ”€ æ˜¯ â†’ ONNX Runtime (CPU Provider)                  â”‚
â”‚    â””â”€ å¦ â†’ ç»§ç»­                                           â”‚
â”‚                                                          â”‚
â”‚  æœ‰åŸå§‹ PyTorch/TF æ¨¡å‹ï¼Ÿ                                â”‚
â”‚    â”œâ”€ æ˜¯ â†’ åŸç”Ÿ CoreMLï¼ˆå¿« 20 å€ï¼‰                      â”‚
â”‚    â””â”€ å¦ â†’ ONNX Runtime + CoreML Provider              â”‚
â”‚                                                          â”‚
â”‚  è¿½æ±‚æè‡´æ€§èƒ½ï¼Ÿ                                           â”‚
â”‚    â”œâ”€ æ˜¯ â†’ åŸç”Ÿ CoreML                                  â”‚
â”‚    â””â”€ å¦ â†’ ONNX Runtime                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æœªæ¥å±•æœ›

- **ONNX Runtime çš„ CoreML Provider è¿˜éœ€ä¼˜åŒ–**ï¼šæœŸå¾…æœªæ¥ç‰ˆæœ¬æ€§èƒ½æå‡
- **ç›´æ¥ ONNX â†’ CoreML è½¬æ¢**ï¼šå¸Œæœ› Apple æˆ–ç¤¾åŒºæä¾›æ›´å¥½çš„å·¥å…·
- **é‡åŒ–å’Œä¼˜åŒ–**ï¼šæ¢ç´¢ FP16/INT8 é‡åŒ–å¯¹æ€§èƒ½çš„å½±å“

---

## å‚è€ƒèµ„æº

- [ONNX Runtime æ–‡æ¡£](https://onnxruntime.ai/)
- [CoreML Tools æ–‡æ¡£](https://coremltools.readme.io/)
- [Apple CoreML å®˜æ–¹æ–‡æ¡£](https://developer.apple.com/documentation/coreml)
